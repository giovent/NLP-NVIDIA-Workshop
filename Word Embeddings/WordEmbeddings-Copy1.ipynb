{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text factual query using Word2Vec with TensorFlow\n",
    "\n",
    "## Intro\n",
    "\n",
    "We can take words in a language as independent set of characters and define their meaning individually. Even though that works for basic meaning, the relationship between each of the words are lost. So, we need to convert them into a space or representation that is easy to manipulate while inter-relational features are preserved. Vector representations are the most commonly used construct for this purpose in Mathematics. Thus, _word embedding_ is a process of mapping of words (or phrases) from the vocabulary to vectors of real numbers. TensorFlow's [Word2Vec](https://www.tensorflow.org/tutorials/word2vec) model is widely used for this purpose. \n",
    "\n",
    "### Suggested readings:\n",
    "* To understand better Skip-Gram Model the following tutorial is suggested\n",
    " - [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "* To know about more complex and effective implementations of word2vec models see\n",
    "\n",
    " - [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start by trying to convert three sentences to dense vectors. As explained by [Word2Vec](https://www.tensorflow.org/tutorials/word2vec) that [Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis) relies on the assumption that words appearing in the same context probably share the semantic meaning. Dense calculations compared to sparse calculations are more efficient so using vectors allow words of similar meaning to appear near each other. Go ahead and run the next cell to get started by importing the necessary libraries and setting the basic sentences for us to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Everything</th>\n",
       "      <th>She</th>\n",
       "      <th>Sky</th>\n",
       "      <th>better</th>\n",
       "      <th>blue</th>\n",
       "      <th>getting</th>\n",
       "      <th>is</th>\n",
       "      <th>possible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Everything</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>She</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sky</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getting</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>possible</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Everything  She  Sky  better  blue  getting  is  possible\n",
       "Everything         1.0  0.0  0.0     0.0   0.0      0.0   1       1.0\n",
       "She                0.0  1.0  0.0     1.0   0.0      1.0   1       0.0\n",
       "Sky                0.0  0.0  1.0     0.0   1.0      0.0   1       0.0\n",
       "better             0.0  1.0  0.0     1.0   0.0      1.0   1       0.0\n",
       "blue               0.0  0.0  1.0     0.0   1.0      0.0   1       0.0\n",
       "getting            0.0  1.0  0.0     1.0   0.0      1.0   1       0.0\n",
       "is                 1.0  1.0  1.0     1.0   1.0      1.0   1       1.0\n",
       "possible           1.0  0.0  0.0     0.0   0.0      0.0   1       1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Let's for example consider a simple way to map words from sentences into dense vectors.\n",
    "# Let's make a table with words coocurrencies and then project vectors of all words into 2D using PCA.\n",
    "\n",
    "s = ['Sky is blue', 'She is getting better', 'Everything is possible']\n",
    "dic = defaultdict(dict)\n",
    "for sent in s:\n",
    "    words = sent.split()\n",
    "    for w in words:\n",
    "        for w2 in words:\n",
    "            dic[w][w2]=1\n",
    "\n",
    "df = pd.DataFrame(dic)\n",
    "df.fillna(0, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the vector space created by every word in each sentence. If they appear in the same sentence then the weight is 1. If they do not appear in the same sentence then it's at 0. The table basically gives relations between of the words given the 3 context sentences.\n",
    "\n",
    "Now go ahead and run the next cell to collapse the words relationship into smaller dimensions so you'll see the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdQAAAGzCAYAAACW1uaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKkBgypJUgMGVZKkBv4/eYFPH6oFz/wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc1e3afd1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = PCA().fit_transform(df)\n",
    "\n",
    "font = {'size'   : 15}\n",
    "plt.rc('font', **font)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(res[:,0], res[:,1])\n",
    "plt.axis('off')\n",
    "for i, label in enumerate(df.columns):\n",
    "    x, y = res[i,0], res[i,1]\n",
    "    plt.scatter(x, y)\n",
    "    annot = {'has': (1, 50), 'is': (1, 5)}\n",
    "    plt.annotate(label, xy=(x, y),\n",
    "                 xytext=annot.get(label,(1+i*2, 6*i)), \n",
    "                 textcoords='offset points',\n",
    "                   ha='right', va='bottom', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Q. Why is the word 'is' by itself?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. Cause it is used in different cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec: skip gram & cbow\n",
    "\n",
    "Models __CBOW (Continuous Bag of Words)__ and __Skip gram__ were invented in the now distant 2013,\n",
    "*article*:\n",
    "[*Tomas Mikolov et al.*](https://arxiv.org/pdf/1301.3781v3.pdf)\n",
    "\n",
    "* __CBOW__ model predict missing word (focus word) using context (surrounding words).\n",
    "* __skip gram__ model is reverse to _CBOW_. It predicts context based on the word in focus.\n",
    "\n",
    "* **Context** is a fixed number of words to the left and right of the word in focus (see picture below). The length of the context is defined by the \"window\" parameter.\n",
    "\n",
    "![context](pics/context.png)\n",
    "\n",
    "Two models comparision\n",
    "\n",
    "![architecture](pics/architecture.png)\n",
    "\n",
    "___\n",
    "\n",
    "There are a lot of implementations of word2vec e.g. [gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb).\n",
    "And there are a lot of trained word-vectors which are already ready to use. But today we will learn how to create your own word embeddings.\n",
    "___\n",
    "\n",
    "\n",
    "### Skip-gram\n",
    "\n",
    "Consider a corpus with a sequence of words $ w_1, w_2, .., w_T $.\n",
    "\n",
    "Objective function (we would like to maximize it) for _skip gram_ is defined as follow:\n",
    "\n",
    "\n",
    "$$ AverageLogProbability = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leqslant j\\leqslant c, j \\neq 0} log\\ p (w_{t+j} | w_t) $$\n",
    "\n",
    "* where $ c $ is a context length.\n",
    "* $w_t$ -- focus word\n",
    "\n",
    "The basic formulation for probability $ p (w_{t+j} | w_t) $ is calculated using __Softmax__ -\n",
    "\n",
    "$$ p (w_o | w_i) = \\frac{exp(s(v_i, v_o))}{ \\sum^{W}_{w=1}  exp(s(v_{w}, v_{i} )) } $$\n",
    "\n",
    "where\n",
    "* $v_i$ and $v_o$ input and output vector representations of $w_i$, $w_o$ .\n",
    "* $s(v_i, v_o) = v^{T} _{o} \\cdot v_{i}$\n",
    "* $W$ is the number of words in vocabulary\n",
    "\n",
    "___\n",
    "\n",
    "### CBOW\n",
    "\n",
    "Predict word using context.\n",
    "\n",
    "$$ E = -log\\ p(w_o\\ |\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c}) $$\n",
    "\n",
    "\n",
    "The **probability** is the same as in the *skip gram* model, but now $v_i$ is a sum of context-word vectors.\n",
    "\n",
    "$$ p(w_o\\ |\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c})  = \\frac{exp(s(v_i, v_o))}{\\sum^{W}_{w=1}  exp(s(v_{w}, v_{i} ))} $$\n",
    "\n",
    "\n",
    "* $\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c}$ -- input context words\n",
    "* $ v_i = \\sum^{c}_{k=1} w_{k}$\n",
    "* $ v_o$ = vector of output word\n",
    "* $s(v_i, v_o) = v^{T} _{o} \\cdot v_{i}$\n",
    "\n",
    "___\n",
    "\n",
    "Let's implement __`CBOW`__ using tf framework.\n",
    "\n",
    "And then implement __`skip gram`__ using CBOW implementation as an example.\n",
    "___\n",
    "\n",
    "First import TensorFlow as we did with other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [text8 dataset](http://mattmahoney.net/dc/textdata).\n",
    "\n",
    "It's a 100 Mb dump of English Wikipedia site at the time of March 3, 2006. It gives us a rich dataset to work with while the size isn't too big yet. \n",
    "\n",
    "# Working with data\n",
    "\n",
    "First we need to prepare the data so we can process it easily. One issue in NLP is the text corpus we have to deal with are usually long. Let's fetch the data but ensure that it's done only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at /notebooks/data/text8.zip.\n"
     ]
    }
   ],
   "source": [
    "# WARNING! if this file \"./data/text8.zip\" doesn't exist\n",
    "# it will be downloaded right now.\n",
    "\n",
    "import os, urllib.request\n",
    "def fetch_data(url):\n",
    "    \n",
    "    filename = url.split(\"/\")[-1]\n",
    "    datadir = os.path.join(os.getcwd(), \"data\")\n",
    "    filepath = os.path.join(datadir, filename)\n",
    "    \n",
    "    if not os.path.exists(datadir):\n",
    "        os.makedirs(datadir)\n",
    "    if not os.path.exists(filepath):\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "filepath = fetch_data(url)\n",
    "print (\"Data at {0}.\".format(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then unzip and read the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_size = 17005207\n"
     ]
    }
   ],
   "source": [
    "import os, zipfile\n",
    "\n",
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "words = read_data(filepath)\n",
    "print(\"data_size = {0}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the first 50K more frequently used words are considered here N = 50000. The rest of the words are marked with unknow token \"UNK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least frequent word:  ('corinne', 9)\n",
      "data: [5236, 3082, 12, 6, 195]\n",
      "count: [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "word_to_index: [('parke', 31703), ('euskara', 48654), ('privileged', 13547), ('eases', 45853), ('ptolemaic', 18099)]\n",
      "index_to_word: [(0, 'UNK'), (1, 'the'), (2, 'of'), (3, 'and'), (4, 'one')]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_dataset(words, vocabulary_size):\n",
    "    count = [[ \"UNK\", -1 ]]\n",
    "    count.extend(Counter(words).most_common(vocabulary_size-1))\n",
    "    print(\"Least frequent word: \", count[-1])\n",
    "    word_to_index = { word: i for i, (word, _) in enumerate(count) }\n",
    "    data = [word_to_index.get(word, 0) for word in words] # map unknown words to 0\n",
    "    unk_count = data.count(0) # Number of unknown words\n",
    "    count[0][1] = unk_count\n",
    "    index_to_word= dict(zip(word_to_index.values(), word_to_index.keys()))\n",
    "    \n",
    "    return data, count, word_to_index, index_to_word\n",
    "\n",
    "vocabulary_size = 50000\n",
    "data, count, word_to_index, index_to_word = build_dataset(words, vocabulary_size)\n",
    "\n",
    "# Everything you need to know about the dataset\n",
    "\n",
    "print(\"data: {0}\".format(data[:5]))\n",
    "print(\"count: {0}\".format(count[:5]))\n",
    "print(\"word_to_index: {0}\".format(list(word_to_index.items())[:5]))\n",
    "print(\"index_to_word: {0}\".format(list(index_to_word.items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Let's look at characteristics of the wiki dataset.\n",
    "\n",
    "Q. How many 'unknown' occurrences are there? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. 418391"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. Which word has the highest occurrences?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What is the index for that word? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the characteristics of the dataset, let's batch up the data and start processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data = ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the']\n",
      "bag_window = 1\n",
      "batch = [['anarchism', 'as'], ['originated', 'a'], ['as', 'term'], ['a', 'of']]\n",
      "labels = ['originated', 'as', 'a', 'term']\n",
      "\n",
      "bag_window = 2\n",
      "batch = [['anarchism', 'originated', 'a', 'term'], ['originated', 'as', 'term', 'of'], ['as', 'a', 'of', 'abuse'], ['a', 'term', 'abuse', 'first']]\n",
      "labels = ['as', 'a', 'term', 'of']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def generate_batch(data_index, data_size, batch_size, bag_window):\n",
    "    span = 2 * bag_window + 1 # [ bag_window, target, bag_window ]\n",
    "    batch = np.ndarray(shape = (batch_size, span - 1), dtype = np.int32)\n",
    "    labels = np.ndarray(shape = (batch_size, 1), dtype = np.int32)\n",
    "    \n",
    "    data_buffer = deque(maxlen = span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        data_buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % data_size\n",
    "        \n",
    "    for i in range(batch_size):\n",
    "        data_list = list(data_buffer)\n",
    "        labels[i, 0] = data_list.pop(bag_window)\n",
    "        batch[i] = data_list\n",
    "        \n",
    "        data_buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % data_size\n",
    "    return data_index, batch, labels\n",
    "\n",
    "\n",
    "print(\"data = {0}\".format([index_to_word[each] for each in data[:16]]))\n",
    "data_index, data_size, batch_size = 0, len(data), 4\n",
    "for bag_window in [1, 2]:\n",
    "    _, batch, labels = generate_batch(data_index, data_size, batch_size, bag_window)\n",
    "    print(\"bag_window = {0}\".format(bag_window))\n",
    "    print(\"batch = {0}\".format([[index_to_word[index] for index in each] for each in batch]))\n",
    "    print(\"labels = {0}\\n\".format([index_to_word[each] for each in labels.reshape(4)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now just take a close look at the output.\n",
    "\n",
    "* We just want to implement _CBOW_, and therefore missed words are considered as the `labels`.\n",
    "\n",
    "* Remember about the __window__ parameter discussed above, here it is __`bag_window`__.\n",
    "\n",
    "* Each sample in the batch has a number of words equal to __`bag_window * 2`__\n",
    "___\n",
    "\n",
    "# CBOW architecture\n",
    "\n",
    "Let's start implementing CBOW architecture. Pay close attention to each of the steps as you'll be having to replicate the process for skip-gram model. The two processes should be very similar to each other. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# define constants\n",
    "batch_size = 256\n",
    "embedding_size = 128\n",
    "\n",
    "# How many words to consider from each side\n",
    "bag_window = 2\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #\n",
    "    # Take the vectors for the context words, which are all bag_window * 2\n",
    "    train_data = tf.placeholder(tf.int32, [batch_size, bag_window * 2])\n",
    "    # Label -- is a word in focus\n",
    "    train_labels = tf.placeholder(tf.int32, [batch_size])\n",
    "    \n",
    "    #\n",
    "    # Create an embedding matrix\n",
    "    # and initialize it by sampling from the uniform distribution [-1, 1]\n",
    "    \n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    #\n",
    "    # Get vectors corresponding to the indices of context words\n",
    "    # embed is a matrix with shape [batch_size, bag_window * 2, embedding_size]\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_data)\n",
    "    \n",
    "    # Sum up all the context vectors to the one vector with the same dimension\n",
    "    # Here we got a matrix of such vectors with the shape [batch_size, embedding_size]\n",
    "    context_sum = tf.reduce_sum(embed, 1)\n",
    "    \n",
    "    scores = tf.matmul(context_sum, embeddings, transpose_b=True)\n",
    "    \n",
    "    one_hot_labels = tf.one_hot(train_labels, vocabulary_size)\n",
    "    loss_tensor = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits=scores)\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "    \n",
    "    # We need to normalize word embeddings for dot product to be a cosine distance \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims = True))\n",
    "    normalized_embeddings = embeddings / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CBOW model\n",
    "\n",
    "For 8k steps on 2 physical CPU cores it takes about 18 minutes.\n",
    "\n",
    "Unfortunately, it is not enough to train good embeddings.\n",
    "\n",
    "Luckily, we have GPU available in this lab, so you could try to achieve reasonable quality much faster. Good representations should take over 50k iterations. Now go ahead and run it to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step = 10, average_loss = 54.553575897216795\n",
      "step = 20, average_loss = 49.23924522399902\n",
      "step = 30, average_loss = 48.740962982177734\n",
      "step = 40, average_loss = 48.502098083496094\n",
      "step = 50, average_loss = 48.36283912658691\n",
      "step = 60, average_loss = 47.937860107421876\n",
      "step = 70, average_loss = 47.37428970336914\n",
      "step = 80, average_loss = 47.799564361572266\n",
      "step = 90, average_loss = 47.048775100708006\n",
      "step = 100, average_loss = 46.70476036071777\n",
      "step = 110, average_loss = 46.665618896484375\n",
      "step = 120, average_loss = 45.302925491333006\n",
      "step = 130, average_loss = 45.07261772155762\n",
      "step = 140, average_loss = 45.9432731628418\n",
      "step = 150, average_loss = 45.07905235290527\n",
      "step = 160, average_loss = 44.94465637207031\n",
      "step = 170, average_loss = 45.125384140014646\n",
      "step = 180, average_loss = 44.11500205993652\n",
      "step = 190, average_loss = 44.23042259216309\n",
      "step = 200, average_loss = 43.161503601074216\n",
      "step = 210, average_loss = 43.5669189453125\n",
      "step = 220, average_loss = 43.657572174072264\n",
      "step = 230, average_loss = 42.580895614624026\n",
      "step = 240, average_loss = 42.142852020263675\n",
      "step = 250, average_loss = 42.18753852844238\n",
      "step = 260, average_loss = 41.16863594055176\n",
      "step = 270, average_loss = 43.08373794555664\n",
      "step = 280, average_loss = 42.26437911987305\n",
      "step = 290, average_loss = 42.37036819458008\n",
      "step = 300, average_loss = 42.200670623779295\n",
      "step = 310, average_loss = 42.1205696105957\n",
      "step = 320, average_loss = 41.72843208312988\n",
      "step = 330, average_loss = 40.87373428344726\n",
      "step = 340, average_loss = 42.39472122192383\n",
      "step = 350, average_loss = 40.6454833984375\n",
      "step = 360, average_loss = 40.3904727935791\n",
      "step = 370, average_loss = 40.268681335449216\n",
      "step = 380, average_loss = 39.331055068969725\n",
      "step = 390, average_loss = 41.01933479309082\n",
      "step = 400, average_loss = 39.71866111755371\n",
      "step = 410, average_loss = 39.88650321960449\n",
      "step = 420, average_loss = 41.27186584472656\n",
      "step = 430, average_loss = 40.788736724853514\n",
      "step = 440, average_loss = 40.126469421386716\n",
      "step = 450, average_loss = 38.662013626098634\n",
      "step = 460, average_loss = 36.92188949584961\n",
      "step = 470, average_loss = 39.585845184326175\n",
      "step = 480, average_loss = 39.96608352661133\n",
      "step = 490, average_loss = 39.455197143554685\n",
      "step = 500, average_loss = 38.535169219970705\n",
      "step = 510, average_loss = 38.43396148681641\n",
      "step = 520, average_loss = 39.104088973999026\n",
      "step = 530, average_loss = 36.79184913635254\n",
      "step = 540, average_loss = 37.7049072265625\n",
      "step = 550, average_loss = 38.296866226196286\n",
      "step = 560, average_loss = 37.91111946105957\n",
      "step = 570, average_loss = 39.091240310668944\n",
      "step = 580, average_loss = 38.61583366394043\n",
      "step = 590, average_loss = 38.1542106628418\n",
      "step = 600, average_loss = 37.73540840148926\n",
      "step = 610, average_loss = 38.284362411499025\n",
      "step = 620, average_loss = 38.89700698852539\n",
      "step = 630, average_loss = 39.21778564453125\n",
      "step = 640, average_loss = 36.49680404663086\n",
      "step = 650, average_loss = 35.110287475585935\n",
      "step = 660, average_loss = 36.47827072143555\n",
      "step = 670, average_loss = 37.26611213684082\n",
      "step = 680, average_loss = 35.79289207458496\n",
      "step = 690, average_loss = 35.032950973510744\n",
      "step = 700, average_loss = 34.82364692687988\n",
      "step = 710, average_loss = 37.39273643493652\n",
      "step = 720, average_loss = 36.69624366760254\n",
      "step = 730, average_loss = 36.08420104980469\n",
      "step = 740, average_loss = 34.454409217834474\n",
      "step = 750, average_loss = 32.4850772857666\n",
      "step = 760, average_loss = 35.007772827148436\n",
      "step = 770, average_loss = 34.63139877319336\n",
      "step = 780, average_loss = 35.53009452819824\n",
      "step = 790, average_loss = 32.32745761871338\n",
      "step = 800, average_loss = 35.67138595581055\n",
      "step = 810, average_loss = 35.90183448791504\n",
      "step = 820, average_loss = 33.2437557220459\n",
      "step = 830, average_loss = 33.700507926940915\n",
      "step = 840, average_loss = 31.91934757232666\n",
      "step = 850, average_loss = 32.129424285888675\n",
      "step = 860, average_loss = 34.448784255981444\n",
      "step = 870, average_loss = 32.196109199523924\n",
      "step = 880, average_loss = 33.52103252410889\n",
      "step = 890, average_loss = 32.61920356750488\n",
      "step = 900, average_loss = 33.39280548095703\n",
      "step = 910, average_loss = 35.98585548400879\n",
      "step = 920, average_loss = 33.32773609161377\n",
      "step = 930, average_loss = 33.81122360229492\n",
      "step = 940, average_loss = 32.14529457092285\n",
      "step = 950, average_loss = 31.28245601654053\n",
      "step = 960, average_loss = 30.60452194213867\n",
      "step = 970, average_loss = 30.821934509277344\n",
      "step = 980, average_loss = 31.202637672424316\n",
      "step = 990, average_loss = 33.25391387939453\n",
      "step = 1000, average_loss = 32.29804916381836\n",
      "step = 1010, average_loss = 32.13046932220459\n",
      "step = 1020, average_loss = 31.700631141662598\n",
      "step = 1030, average_loss = 31.114355659484865\n",
      "step = 1040, average_loss = 30.813419342041016\n",
      "step = 1050, average_loss = 32.664605903625485\n",
      "step = 1060, average_loss = 33.54713516235351\n",
      "step = 1070, average_loss = 32.530435180664064\n",
      "step = 1080, average_loss = 33.07285556793213\n",
      "step = 1090, average_loss = 32.287118911743164\n",
      "step = 1100, average_loss = 32.027328681945804\n",
      "step = 1110, average_loss = 28.297175788879393\n",
      "step = 1120, average_loss = 32.0474796295166\n",
      "step = 1130, average_loss = 32.28009967803955\n",
      "step = 1140, average_loss = 31.053331756591795\n",
      "step = 1150, average_loss = 30.384471702575684\n",
      "step = 1160, average_loss = 29.463136863708495\n",
      "step = 1170, average_loss = 27.933627319335937\n",
      "step = 1180, average_loss = 30.23711242675781\n",
      "step = 1190, average_loss = 29.072355461120605\n",
      "step = 1200, average_loss = 30.162276649475096\n",
      "step = 1210, average_loss = 32.87367687225342\n",
      "step = 1220, average_loss = 31.178019714355468\n",
      "step = 1230, average_loss = 30.780944061279296\n",
      "step = 1240, average_loss = 30.4517879486084\n",
      "step = 1250, average_loss = 29.948381996154787\n",
      "step = 1260, average_loss = 28.486377906799316\n",
      "step = 1270, average_loss = 28.875178718566893\n",
      "step = 1280, average_loss = 28.830121421813963\n",
      "step = 1290, average_loss = 29.623772048950194\n",
      "step = 1300, average_loss = 30.618071174621583\n",
      "step = 1310, average_loss = 29.698822212219238\n",
      "step = 1320, average_loss = 30.43449306488037\n",
      "step = 1330, average_loss = 31.500092887878417\n",
      "step = 1340, average_loss = 31.063697242736815\n",
      "step = 1350, average_loss = 29.983733940124512\n",
      "step = 1360, average_loss = 29.4982120513916\n",
      "step = 1370, average_loss = 29.95622386932373\n",
      "step = 1380, average_loss = 29.705930137634276\n",
      "step = 1390, average_loss = 29.23076801300049\n",
      "step = 1400, average_loss = 28.98273983001709\n",
      "step = 1410, average_loss = 29.160826873779296\n",
      "step = 1420, average_loss = 28.271257209777833\n",
      "step = 1430, average_loss = 28.466289520263672\n",
      "step = 1440, average_loss = 29.309996604919434\n",
      "step = 1450, average_loss = 25.879260063171387\n",
      "step = 1460, average_loss = 30.26122760772705\n",
      "step = 1470, average_loss = 27.658337593078613\n",
      "step = 1480, average_loss = 26.922593116760254\n",
      "step = 1490, average_loss = 27.80062198638916\n",
      "step = 1500, average_loss = 28.04803581237793\n",
      "step = 1510, average_loss = 29.61462287902832\n",
      "step = 1520, average_loss = 25.91747989654541\n",
      "step = 1530, average_loss = 28.19317569732666\n",
      "step = 1540, average_loss = 27.28202304840088\n",
      "step = 1550, average_loss = 26.190596961975096\n",
      "step = 1560, average_loss = 27.71234264373779\n",
      "step = 1570, average_loss = 27.063631057739258\n",
      "step = 1580, average_loss = 26.533721733093262\n",
      "step = 1590, average_loss = 27.50603370666504\n",
      "step = 1600, average_loss = 27.069885444641113\n",
      "step = 1610, average_loss = 28.150196075439453\n",
      "step = 1620, average_loss = 26.599478340148927\n",
      "step = 1630, average_loss = 25.392413330078124\n",
      "step = 1640, average_loss = 27.560215377807616\n",
      "step = 1650, average_loss = 29.1861083984375\n",
      "step = 1660, average_loss = 28.593878746032715\n",
      "step = 1670, average_loss = 29.061300468444824\n",
      "step = 1680, average_loss = 26.80769786834717\n",
      "step = 1690, average_loss = 26.2802734375\n",
      "step = 1700, average_loss = 25.94503688812256\n",
      "step = 1710, average_loss = 25.393584060668946\n",
      "step = 1720, average_loss = 24.035047340393067\n",
      "step = 1730, average_loss = 26.784105491638183\n",
      "step = 1740, average_loss = 24.087720108032226\n",
      "step = 1750, average_loss = 23.989616203308106\n",
      "step = 1760, average_loss = 26.591678047180174\n",
      "step = 1770, average_loss = 25.126874351501463\n",
      "step = 1780, average_loss = 25.46276798248291\n",
      "step = 1790, average_loss = 25.556758880615234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1800, average_loss = 22.891542625427245\n",
      "step = 1810, average_loss = 25.893312072753908\n",
      "step = 1820, average_loss = 26.59531173706055\n",
      "step = 1830, average_loss = 27.241184043884278\n",
      "step = 1840, average_loss = 27.271404838562013\n",
      "step = 1850, average_loss = 26.852913284301756\n",
      "step = 1860, average_loss = 24.70778331756592\n",
      "step = 1870, average_loss = 25.2056001663208\n",
      "step = 1880, average_loss = 25.758976936340332\n",
      "step = 1890, average_loss = 27.06750144958496\n",
      "step = 1900, average_loss = 24.39186191558838\n",
      "step = 1910, average_loss = 27.15128860473633\n",
      "step = 1920, average_loss = 26.422350120544433\n",
      "step = 1930, average_loss = 25.87488708496094\n",
      "step = 1940, average_loss = 26.096927452087403\n",
      "step = 1950, average_loss = 24.029486656188965\n",
      "step = 1960, average_loss = 25.88885364532471\n",
      "step = 1970, average_loss = 25.75948848724365\n",
      "step = 1980, average_loss = 23.642441749572754\n",
      "step = 1990, average_loss = 22.40945510864258\n",
      "step = 2000, average_loss = 20.436109733581542\n",
      "step = 2010, average_loss = 24.526982688903807\n",
      "step = 2020, average_loss = 25.073300743103026\n",
      "step = 2030, average_loss = 25.239418601989748\n",
      "step = 2040, average_loss = 21.696344947814943\n",
      "step = 2050, average_loss = 25.601171875\n",
      "step = 2060, average_loss = 24.559151458740235\n",
      "step = 2070, average_loss = 20.09811611175537\n",
      "step = 2080, average_loss = 20.215369987487794\n",
      "step = 2090, average_loss = 19.79422092437744\n",
      "step = 2100, average_loss = 18.884958839416505\n",
      "step = 2110, average_loss = 17.9440354347229\n",
      "step = 2120, average_loss = 24.758956909179688\n",
      "step = 2130, average_loss = 22.40653438568115\n",
      "step = 2140, average_loss = 19.82164192199707\n",
      "step = 2150, average_loss = 22.221405029296875\n",
      "step = 2160, average_loss = 23.411284065246583\n",
      "step = 2170, average_loss = 26.65788516998291\n",
      "step = 2180, average_loss = 26.436842346191405\n",
      "step = 2190, average_loss = 24.44800720214844\n",
      "step = 2200, average_loss = 24.80181465148926\n",
      "step = 2210, average_loss = 19.649386405944824\n",
      "step = 2220, average_loss = 22.717501640319824\n",
      "step = 2230, average_loss = 24.790234184265138\n",
      "step = 2240, average_loss = 22.822528076171874\n",
      "step = 2250, average_loss = 21.318908500671387\n",
      "step = 2260, average_loss = 25.446762084960938\n",
      "step = 2270, average_loss = 23.75968017578125\n",
      "step = 2280, average_loss = 22.869339561462404\n",
      "step = 2290, average_loss = 23.00652961730957\n",
      "step = 2300, average_loss = 21.538209438323975\n",
      "step = 2310, average_loss = 22.47534923553467\n",
      "step = 2320, average_loss = 22.254188537597656\n",
      "step = 2330, average_loss = 22.67152671813965\n",
      "step = 2340, average_loss = 21.90974655151367\n",
      "step = 2350, average_loss = 20.939949989318848\n",
      "step = 2360, average_loss = 21.837936782836913\n",
      "step = 2370, average_loss = 20.05867729187012\n",
      "step = 2380, average_loss = 21.346306610107423\n",
      "step = 2390, average_loss = 21.700895690917967\n",
      "step = 2400, average_loss = 19.620078754425048\n",
      "step = 2410, average_loss = 20.08145809173584\n",
      "step = 2420, average_loss = 23.105085182189942\n",
      "step = 2430, average_loss = 22.59582633972168\n",
      "step = 2440, average_loss = 23.218762588500976\n",
      "step = 2450, average_loss = 22.488056945800782\n",
      "step = 2460, average_loss = 20.957929420471192\n",
      "step = 2470, average_loss = 22.6177490234375\n",
      "step = 2480, average_loss = 21.892793273925783\n",
      "step = 2490, average_loss = 19.492086791992186\n",
      "step = 2500, average_loss = 22.875674057006837\n",
      "step = 2510, average_loss = 24.556095314025878\n",
      "step = 2520, average_loss = 21.821968269348144\n",
      "step = 2530, average_loss = 23.212828636169434\n",
      "step = 2540, average_loss = 23.361656379699706\n",
      "step = 2550, average_loss = 24.452684593200683\n",
      "step = 2560, average_loss = 17.890589427947997\n",
      "step = 2570, average_loss = 21.881863403320313\n",
      "step = 2580, average_loss = 22.522078704833984\n",
      "step = 2590, average_loss = 22.05576858520508\n",
      "step = 2600, average_loss = 23.624630546569826\n",
      "step = 2610, average_loss = 22.511287879943847\n",
      "step = 2620, average_loss = 22.703560638427735\n",
      "step = 2630, average_loss = 22.438069534301757\n",
      "step = 2640, average_loss = 21.72185859680176\n",
      "step = 2650, average_loss = 21.408242607116698\n",
      "step = 2660, average_loss = 20.44533176422119\n",
      "step = 2670, average_loss = 20.102459144592284\n",
      "step = 2680, average_loss = 22.17274284362793\n",
      "step = 2690, average_loss = 23.159619903564455\n",
      "step = 2700, average_loss = 20.116546821594238\n",
      "step = 2710, average_loss = 20.06653003692627\n",
      "step = 2720, average_loss = 20.692705345153808\n",
      "step = 2730, average_loss = 20.244480323791503\n",
      "step = 2740, average_loss = 19.38561382293701\n",
      "step = 2750, average_loss = 20.019525909423827\n",
      "step = 2760, average_loss = 19.685926628112792\n",
      "step = 2770, average_loss = 22.174775886535645\n",
      "step = 2780, average_loss = 20.11433048248291\n",
      "step = 2790, average_loss = 19.029107570648193\n",
      "step = 2800, average_loss = 21.26817798614502\n",
      "step = 2810, average_loss = 20.238564491271973\n",
      "step = 2820, average_loss = 19.101610946655274\n",
      "step = 2830, average_loss = 19.046465969085695\n",
      "step = 2840, average_loss = 17.951339054107667\n",
      "step = 2850, average_loss = 18.000399589538574\n",
      "step = 2860, average_loss = 20.329127883911134\n",
      "step = 2870, average_loss = 21.2600679397583\n",
      "step = 2880, average_loss = 20.48294105529785\n",
      "step = 2890, average_loss = 20.273571586608888\n",
      "step = 2900, average_loss = 20.11202392578125\n",
      "step = 2910, average_loss = 19.0746150970459\n",
      "step = 2920, average_loss = 18.71360054016113\n",
      "step = 2930, average_loss = 17.981457138061522\n",
      "step = 2940, average_loss = 16.873258304595947\n",
      "step = 2950, average_loss = 19.445832633972167\n",
      "step = 2960, average_loss = 19.098072910308836\n",
      "step = 2970, average_loss = 23.47932195663452\n",
      "step = 2980, average_loss = 22.348514366149903\n",
      "step = 2990, average_loss = 21.645503044128418\n",
      "step = 3000, average_loss = 20.562780952453615\n",
      "step = 3010, average_loss = 21.165585708618163\n",
      "step = 3020, average_loss = 19.62373580932617\n",
      "step = 3030, average_loss = 20.508094215393065\n",
      "step = 3040, average_loss = 19.584914016723634\n",
      "step = 3050, average_loss = 21.066198539733886\n",
      "step = 3060, average_loss = 21.390642166137695\n",
      "step = 3070, average_loss = 19.297589778900146\n",
      "step = 3080, average_loss = 14.951452350616455\n",
      "step = 3090, average_loss = 18.019548892974854\n",
      "step = 3100, average_loss = 22.551885986328124\n",
      "step = 3110, average_loss = 20.430282211303712\n",
      "step = 3120, average_loss = 20.00841999053955\n",
      "step = 3130, average_loss = 18.23772087097168\n",
      "step = 3140, average_loss = 20.178075218200682\n",
      "step = 3150, average_loss = 19.57449789047241\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#num_steps = 50001\n",
    "num_steps = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    try:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        average_loss = 0\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            data_index, batch, labels = generate_batch(data_index, data_size, batch_size, bag_window)\n",
    "            feed_dict = { train_data: batch, train_labels: labels.reshape(-1) }\n",
    "            _, current_loss = sess.run([optimizer, loss], feed_dict = feed_dict)\n",
    "            average_loss += current_loss\n",
    "            if step % 10 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss = average_loss / 10\n",
    "                    print (\"step = {0}, average_loss = {1}\".format(step, average_loss))\n",
    "                    average_loss = 0\n",
    "    except KeyboardInterrupt:\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "We can use [projector](http://projector.tensorflow.org/) to visualize the results in a word cloud. Go ahead and click on [projector](http://projector.tensorflow.org/) to launch the visualization.\n",
    "\n",
    "### Click on [Projector](http://projector.tensorflow.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('embeddings.txt', 'w') as f:\n",
    "    for n in range(vocabulary_size):\n",
    "        s = '\\t'.join([index_to_word[n]] + [str(num) for num in final_embeddings[n]])\n",
    "        f.write(s + '\\n')\n",
    "with open('mentadata.txt', 'w') as f:\n",
    "    for n in range(vocabulary_size):\n",
    "        f.write(index_to_word[n] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or see TSNE here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "num_points = 350\n",
    "\n",
    "tsne = TSNE(perplexity=10, n_components=2, init=\"pca\", n_iter=5000)\n",
    "two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "words = [index_to_word[i] for i in range(1, num_points+1)]\n",
    "\n",
    "for i, label in enumerate(words):\n",
    "    x, y = two_d_embeddings[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords=\"offset points\",\n",
    "                   ha=\"right\", va=\"bottom\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Your task is to implement `skip-gram` model, using code above.\n",
    "\n",
    "This approach is nicely illustrated with this figure:\n",
    "\n",
    "![skip_gram](./pics/training_data.png)\n",
    "As you can see on the picture, the training set consists of pairs (`central word`, `context word`).\n",
    "\n",
    "I.e. our model takes `central word` and should produce class in softmax, which corresponds to `context word`.\n",
    "\n",
    "The difference between two models is not that big after all, so good luck with coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have implemented batch generator for you\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def generate_batch_2(data_index, data_size, batch_size, num_skips, skip_window):\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape = batch_size, dtype = np.int32)\n",
    "    labels = np.ndarray(shape = (batch_size, 1), dtype = np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "    data_buffer = deque(maxlen = span)\n",
    "    for _ in range(span):\n",
    "        data_buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % data_size\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target, targets_to_avoid = skip_window, [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid: \n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = data_buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = data_buffer[target]\n",
    "        data_buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % data_size\n",
    "    return data_index, batch, labels\n",
    "\n",
    "\n",
    "print (\"data = {0}\\n\".format([index_to_word[each] for each in data[:32]]))\n",
    "data_index, data_size = 0, len(data)\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    data_index, batch, labels = generate_batch_2(data_index=data_index, \n",
    "                                               data_size=data_size, \n",
    "                                               batch_size=16, \n",
    "                                               num_skips=num_skips, \n",
    "                                               skip_window=skip_window)\n",
    "    print (\"data_index = {0}, num_skips = {1}, skip_window = {2}\".format( data_index, num_skips, skip_window))\n",
    "    print (\"batch = {0}\".format([index_to_word[each] for each in batch]))\n",
    "    print (\"labels = {0}\\n\".format([index_to_word[each] for each in labels.reshape(16)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have shown how to implement CBOW model, you have all the tools to create skip-gram model yourself. In the following cell, create skip-gram model by following the steps from CBOW implementation.\n",
    "\n",
    "### Exercise 3\n",
    "Fill-in the following cell with TensorFlow code for skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Once the model is created, next step is train it. Fill-in Tensorflow code for training of skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Finally, after training we want to visualize the results. Implement tSNE visualization of embeddings for skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our embeddings on some real case: we create simple Wikipedia search engine. \n",
    "\n",
    "To do that first of all we need to download Wikipedia sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! wget -c \"https://s3.amazonaws.com/fair-data/starspace/wikipedia_devtst.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! tar -xzvf wikipedia_devtst.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! head wikipedia_test_basedocs.txt -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will vectorize all the articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(text):\n",
    "    tokens = text.lower().split()\n",
    "    num_words = 0\n",
    "    doc_vector = np.zeros_like(final_embeddings[0])\n",
    "    for token in tokens: \n",
    "        if token in word_to_index:\n",
    "            num_words += 1\n",
    "            doc_vector += final_embeddings[word_to_index[token]]\n",
    "    doc_vector /= num_words\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_docs = []\n",
    "docs = []\n",
    "with open(\"wikipedia_test_basedocs.txt\",encoding='utf8') as f:\n",
    "    for doc in f:\n",
    "        doc_vector = vectorize(doc)\n",
    "        vectorized_docs.append(doc_vector)\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our toy search engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    query_vector = vectorize(query)\n",
    "    ranking = []\n",
    "    for i in range(len(vectorized_docs)):\n",
    "        score = np.dot(query_vector, vectorized_docs[i])\n",
    "        ranking.append((score, i))\n",
    "    ranking.sort(key=lambda x: -x[0]) # to have descending sorting\n",
    "    return docs[ranking[0][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at last we could test it on some query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"japanese strong gull\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Create your own query and determine the quality of the results.\n",
    "\n",
    "You could spot that results are not the best, so you could improve quality of the embeddings, by increasing windows size in training, increasing batch size and train longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
