{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Keras\n",
    "Author: [Valentin Malykh](http://val.maly.hk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start our Natural Language Processing(NLP) journey with classification, because it is one basic steps in understanding natural languages yet still very practical. Once we can figure out the meaning of a word then more complex tasks are possible such as sentiment analysis. Knowing sentiment of the words are very useful and common for many industries. For example, online reviews or comments are a common way for any big company to track their public image and how customers feel about them. \n",
    "\n",
    "## Terms\n",
    "First, let us define technical terms used in NLP to describe the inputs we need to parse. \n",
    "\n",
    "*token* - a unit of text, it could be a word (and almost always is), but also it could be a group of words like \"New York\", or sub-word like \"mega\" in \"megabyte\"\n",
    "\n",
    "*document* - sequence of tokens, this could be whole book or a tweet, pedending on a task\n",
    "\n",
    "*corpus* - set of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Steps\n",
    "\n",
    "We will assign each document in a corpus to come class to perform *text classification* for *sentiment analysis*. The task is to breakdown whether a document conveys 3 different sentiments: \"positive\", \"negative\" or \"neutral\". The idea is to understand overall context of the word used along with the meaning to figure out the emotional tone of the document whether it'd be a comment or a paragraph. It's not possible to do the analysis of the world document in one shot but have to be broken down. The strategy we will employ is *part of speech tagging* or simply *PoS-tagging*; a markup of a sentence by PoS for every word. These tags for every word then can be used to feed into doing overall task such as *text classificatioon* we will do in this lab.\n",
    "\n",
    "Now, let us import all the libraries to setup our text classification process. We will be utilizing Keras framework for convenience and utilities like numpy for ease of use. Go ahead and run the following cell to bring in the proper libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a corpus to do text classification on. So let's get download Sentiment Tree Bank from Stanford's [NLP](https://nlp.stanford.edu/sentiment/) group. They describe the complexity of sentiment analysis with their work as following:\n",
    "\n",
    "\"Most sentiment prediction systems work just by looking at words in isolation, giving positive points for positive words and negative points for negative words and then summing up these points. That way, the order of words is ignored and important information is lost. In constrast, our new deep learning model actually builds up a representation of whole sentences based on the sentence structure. It computes the sentiment based on how words compose the meaning of longer phrases. This way, the model is not as easily fooled as previous models. For example, our model learned that funny and witty are positive but the following sentence is still negative overall:\n",
    "\n",
    "*This movie was actually neither that funny, nor super witty.*\"\n",
    "\n",
    "Now let's execute the following two cells to download the [dataset](https://nlp.stanford.edu/sentiment/treebank.html) and unzip it into our workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! if [ ! -f stanfordSentimentTreebank.zip ]; then wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! unzip stanfordSentimentTreebank.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pandas\n",
    "We need help to parse the dataset and don't want to do it manually. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools. \n",
    "\n",
    "So let's import pandas into our environment and read CSV data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "split = pandas.read_csv(\"stanfordSentimentTreebank/datasetSplit.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do a read for pandas, it creates a base object in it called DataFrame. DataFrame is representated as numpy array internally and thus, have some interesting properties. We can exploit some of those properties with the following code to manipulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = pandas.read_csv(\"stanfordSentimentTreebank/datasetSentences.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're using another property of DataFrame - column access ```sentences[\"sentence\"]``` which will return only one specific column of this particular DataFrame. ```tolist()``` obviously returns a python list instead of Series (another base class in pandas). Execute the following cells to prepare the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_labels(sentences):\n",
    "    dictionary = dict()\n",
    "    with open(\"stanfordSentimentTreebank/dictionary.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            splitted = line.split(\"|\")\n",
    "            dictionary[splitted[0].lower()] = int(splitted[1])\n",
    "\n",
    "\n",
    "    labels = [0.5] * (max(dictionary.values()) + 1)\n",
    "    with open(\"stanfordSentimentTreebank/sentiment_labels.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            splitted = line.split(\"|\")\n",
    "            labels[int(splitted[0])] = float(splitted[1])\n",
    "\n",
    "    sent_labels = [0.5] * len(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        full_sent = sentences[i].replace(\"-lrb-\", \"(\").replace(\"-rrb-\", \")\").replace(\"\\\\\\\\\", \"\")\n",
    "        try:\n",
    "            sent_labels[i] = labels[dictionary[full_sent.lower()]]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return sent_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create labels and check how many sentences there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sent_labels(sentences=sentences[\"sentence\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty good dataset for us to start working with. Note that ```concat``` will concatinate DataFrames (and Series) even if they are of different lengths. This flexibility is another reason we are utilizing Panda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pandas.concat([sentences, pandas.DataFrame(labels), split], axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we at first select some columns by their names - ```dataset[[\"sentence\",0,\"splitset_label\"]]```, and after that filtering the produced DataFrame by value of one of its columns ```d[d[\"splitset_label\"] == 1]```.\n",
    "\n",
    "Also, if you call a DataFrame in jupyter, it is an equivalent of ```head()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset[[\"sentence\",0,\"splitset_label\"]]\n",
    "d[d[\"splitset_label\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 20\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to split the dataset into 3 sets: training, validation and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "df_train = d[d[\"splitset_label\"] == 1]\n",
    "df_test = d[d[\"splitset_label\"] == 2]\n",
    "df_val = d[d[\"splitset_label\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Tokenizer``` class from Keras is implementing TF-IDF method of text analysis on provided corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "*term frequency* or *TF*: \n",
    "$$TF(w, d) = \\frac{count(w, d)}{\\sum_{v \\in V}count(v, d)}$$\n",
    "where $w, v$ are tokens (words), $V$ - vocabulary, $d$ - document in corpus\n",
    "\n",
    "*inversed document frequency* or *IDF*:\n",
    "$$IDF(w) = log \\frac{|D|}{\\sum_{d \\in D}\\mathbb{1}(w, d)} $$\n",
    "where $D$ is a corpus, $\\mathbb{1}$ is an indicator function of presence of specific token in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing the Tokenizer...\")\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_train[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've fitted the ```Tokenizer``` on our corpus, it can create a matrix representation of texts. One dimension of the matrix will be number of a text, and other one will be TF-IDF weights of words in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vectorizing sequence data...')\n",
    "x_train = tokenizer.texts_to_matrix(df_train[\"sentence\"], mode='binary')\n",
    "x_test = tokenizer.texts_to_matrix(df_test[\"sentence\"], mode='binary')\n",
    "x_val = tokenizer.texts_to_matrix(df_val[\"sentence\"], mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('x_val shape:', x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train =df_train[0].copy()\n",
    "df_test =df_test[0].copy()\n",
    "df_val =df_val[0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a matrix for our labels. One dimension again will be number of text, but the other one is little bit tricky: we need to produce one-hot encoding for the labels. One-hot encoding will be here zero vector by lenght of number of classes with one at position which correspond to actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train[df_train >= 0.5] = 1.\n",
    "df_train[df_train < 0.5] = 0.\n",
    "\n",
    "df_test[df_test >= 0.5] = 1.\n",
    "df_test[df_test < 0.5] = 0.\n",
    "\n",
    "df_val[df_val >= 0.5] = 1.\n",
    "df_val[df_val < 0.5] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(df_train, num_classes)\n",
    "\n",
    "y_test = keras.utils.to_categorical(df_test, num_classes)\n",
    "\n",
    "y_val = keras.utils.to_categorical(df_val, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.where(y_train[:,1] == 0)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a model in Keras. This model will consist of two ```Dense``` layers and some non-linear function, which is called ```Activation```. ```Dense``` layer is just matrix multiplication, and nothing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we start we need to discuss two more things. First one is Rectified Linear Unit or just __ReLU__. This is common nonlinearity, which is defined by simple formula:\n",
    "$$ReLU(z) = max(0, z)$$\n",
    "Here is its graphical representation (and also sigmoid for comparison):\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*XxxiA0jJvPrHEJHD4z893g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also just a remainder about SoftMax function we'll be using later in this lab:\n",
    "$$SoftMax(x_i)=\\frac{e^{x_i}}{\\sum_{j=1..N}e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building Fully-Connected...')\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "\n",
    "yaml_string = model.to_yaml()\n",
    "model = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to draw our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final touch: loss function for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teaching the network at last!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard  \n",
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click [here](/tensorboard/) to start TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score2[0])\n",
    "print('Test accuracy:', score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict =  model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "y_test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_y_p=y_test_predict[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_y_p[conf_y_p >= 0.5] = 1\n",
    "conf_y_p[conf_y_p <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_y_test = y_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_test = confusion_matrix(conf_y_test, conf_y_p)\n",
    "confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu= (confusion_test[0][0]+confusion_test[1][1])/confusion_test.sum()\n",
    "neg_prec = confusion_test[0][0]/(confusion_test[0][0]+confusion_test[1][0])\n",
    "neg_recall = confusion_test[0][0]/confusion_test[0].sum()\n",
    "print('accu : ',accu)\n",
    "print('neg_prec : ',neg_prec)\n",
    "print('neg_recall : ',neg_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you may see this network isn't that great at this task. So we propose you to get acquinted with recurrent neural networks, which are now industry standard for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/759/1*UkI9za9zTR-HL8uM15Wmzw.png)\n",
    "\n",
    "DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "Formalae:\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1)\n",
    "\n",
    "\n",
    "And on figure:\n",
    "![](https://www.researchgate.net/profile/Marijn_Stollenga/publication/304346489/figure/fig13/AS:376211038588933@1466707109201/Figure-74-RNN-and-LSTM-A-graphical-representation-of-the-RNN-and-LSTM-networks-are.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have no need in the matrices described above, so we just use the embedded version of the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.layers import GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pad (or trim) sentences to maxlen we want for a RNN to be able work with them in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we introduce two new layers: ```Embedding``` - the layer which learn a vector for each word, and ```LSTM``` - which is just an LSTM cell described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Embedding(max_words,128 , dropout=0.2))\n",
    "#model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary() \n",
    "\n",
    "#See this : https://gaussic.github.io/2017/03/03/imdb-sentiment-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the followig training cell for the number of epochs we have specified. This will take about 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    #callbacks=[tensorboard, early_stopping]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('\\n')\n",
    "print('model.metrics_names:', model.metrics_names)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2= model.predict(x_test, batch_size=batch_size)\n",
    "print('Sentiment value:', score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('label:', y_test)\n",
    "y_test = y_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score2[score2>=0.5] = 1.\n",
    "score2[score2 <0.5] = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_test = confusion_matrix(y_test, score2)\n",
    "confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu= (confusion_test[0][0]+confusion_test[1][1])/confusion_test.sum()\n",
    "neg_prec = confusion_test[0][0]/(confusion_test[0][0]+confusion_test[1][0])\n",
    "neg_recall = confusion_test[0][0]/confusion_test[0].sum()\n",
    "print('accu : ',accu)\n",
    "print('neg_prec : ',neg_prec)\n",
    "print('neg_recall : ',neg_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "The score is better, but not much. You can improve it dramatically, just try add some layers, or tweak some hyperparams. Be creative! Your goal is to reach 0.75 on this dataset, but it is not the maximum achievable limit, just a metric in the time you have to complete this lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we can do text classification then many identification tasks open up to apply the same approach on. One example is [MBTI][https://www.kaggle.com/datasnaek/mbti-type) where people's personalities can be divided into 16 different types. The dataset includes writing samples from each of the personality types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Take home exercise to use the kaggle dataset to see if people's personality can be discerned based on the online written samples."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
